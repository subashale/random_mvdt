{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ujson as json\n",
    "#import json\n",
    "import pymongo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mongodb driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "# datasets = \"2newgroup,imbd,quora\"\n",
    "# k = 5 #20%test\n",
    "# vector_sizes = [10, 25, 50, 75, 100]\n",
    "# algorithms_list = ['lr_mvdt', 'rs_mvdt']\n",
    "# epochs_list = [100, 300, 500, 800, 1000]\n",
    "# n_features_list = [2, 5, 10, 20, all]  # should below min vector_size\n",
    "# depth_list = [3, 6, 9, 12, 15]\n",
    "# min_leaf_point_list = [5, 10, 15, 20, 30]\n",
    "\n",
    "datasets = \"20ng_CG_RM,20newsgroup\"#\"imdb\"\n",
    "k = 2\n",
    "vector_sizes = [10, 25]\n",
    "algorithms_list = ['lr_mvdt', 'rs_mvdt']\n",
    "epochs_list = [50, 100]\n",
    "n_features_list = [2, all]\n",
    "min_leaf_point_list = [5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client.thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['result']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.list_collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.result.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(dataLocation):\n",
    "    \"\"\"\n",
    "   Reading data and returning in proper format\n",
    "   :param dataLocation: location of data\n",
    "   :return: set of features, labels and combine\n",
    "   \"\"\"\n",
    "    df = pd.read_csv(dataLocation)\n",
    "\n",
    "    X = np.array(df[df.columns[:-1]].values.tolist(), dtype=np.float64)\n",
    "    y = np.array(df[df.columns[-1]].values.tolist())\n",
    "    return [X, y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_by_n_features_lrrs():\n",
    "    for name in datasets.split(\",\"):\n",
    "        for n in range(1, k+1):        \n",
    "            count = str(n)\n",
    "            for vector_size in vector_sizes:\n",
    "                for algorithm in algorithms_list:                    \n",
    "                    get_average_by_n_features(name, n, vector_size, algorithm)\n",
    "                    for epochs in epochs_list:                                                                   \n",
    "                        get_average_by_n_features(name, n, vector_size, algorithm, epochs)\n",
    "                        for min_leaf_point in min_leaf_point_list:                            \n",
    "                            get_average_by_n_features(name, n, vector_size, algorithm, epochs, min_leaf_point)\n",
    "                            for n_features in n_features_list: \n",
    "                                if n_features == all:                                \n",
    "                                    data_loc = \"data/\"+name+\"/k\"+count+\"/final/\"\n",
    "                                    data_train = str(vector_size) + \"D_\" + name + \"_k\" + count + \"_train.csv\"                                \n",
    "\n",
    "                                    train = read_data(data_loc + data_train)                                \n",
    "                                    n_features = len(train[0][0])\n",
    "\n",
    "                                total += 1        \n",
    "                                #print(name, n, vector_size, algorithm, epochs, min_leaf_point,n_features)\n",
    "                                get_average_by_n_features(name, n, vector_size, algorithm, epochs, min_leaf_point,n_features)\n",
    "    print(total)\n",
    "    \n",
    "# def avg_by_n_features_cart():\n",
    "#     for name in datasets.split(\",\"):\n",
    "#         for n in range(1, k+1):        \n",
    "#             count = str(n)\n",
    "#             for vector_size in vector_sizes:                                                                                                             \n",
    "#                 for min_leaf_point in min_leaf_point_list:                            \n",
    "#                     for n_features in n_features_list: \n",
    "#                         total += 1        \n",
    "#                         #print(name, n, vector_size, algorithm, epochs, min_leaf_point,n_features)\n",
    "#                         get_average_by_n_features(name, n, vector_size, algorithm, epochs, min_leaf_point,n_features)                \n",
    "#     print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "GeeksforGeeks\n"
     ]
    }
   ],
   "source": [
    "# Python program to illustrate \n",
    "# *args for variable number of arguments \n",
    "def myFun(*argv): \n",
    "    print(len(argv))\n",
    "    print(argv[-1])\n",
    "    \n",
    "myFun('Hello', 'Welcome', 'to', 'GeeksforGeeks') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argument order\n",
    "#name, n, vector_size, algorithm, epochs, min_leaf_point,n_features\n",
    "def get_average_by_n_features(*argv):\n",
    "    if len(argv) == 7:\n",
    "        # average by n_features in 10 runs\n",
    "        query = [{'$match': {'dataset': name, 'k_fold': n, 'd2v_vec_size':vector_size,\n",
    "                                                 'algorithm':algorithm, 'epochs':epochs, 'feature_size':n_features,\n",
    "                                                 'min_leaf_point':min_leaf_point \n",
    "                                                }\n",
    "                                     },\n",
    "                 {'$group': {'_id':'$min_leaf_point', 'count': {'$sum':1},\n",
    "                            'train_acc': { '$avg': {'$arrayElemAt': [ '$accuracy', 0 ]}},\n",
    "                            'test_acc': { '$avg': { '$arrayElemAt': ['$accuracy', 1]}},\n",
    "                             'train_pre': { '$avg': { '$arrayElemAt': ['$precision', 0]}},\n",
    "                             'test_pre': { '$avg': { '$arrayElemAt': ['$precision', 1]}},\n",
    "                             'train_rec': { '$avg': { '$arrayElemAt': ['$recall', 0]}},\n",
    "                             'test_rec': { '$avg': { '$arrayElemAt': ['$recall', 1]}},\n",
    "                             'train_f1': { '$avg': { '$arrayElemAt': ['$f1', 0]}},\n",
    "                             'test_f1': { '$avg': { '$arrayElemAt': ['$f1', 1]}},\n",
    "                            }\n",
    "                 }]\n",
    "        #print(query)\n",
    "        d = db.result.aggregate(query)                                \n",
    "        for i in d:\n",
    "            #print(i)\n",
    "            new_result = {'dataset': name,  # name of dataset                  \n",
    "                          'k_fold': int(n),  # which K_fold number\n",
    "                          'd2v_vec_size': int(vector_size),  # dod2vec feature dimension\n",
    "                          'algorithm': algorithm,  # algorithm name\n",
    "                          'epochs': epochs,  # no of epochs\n",
    "                          'min_leaf_point': min_leaf_point,  # given depth of tree\n",
    "                          'feature_size': n_features,  # how may features are taken                  \n",
    "                          'accuracy': [train_acc, test_acc],\n",
    "                          'precision': [train_pre, test_pre], # precision of both train and test                      \n",
    "                          'recall': [train_rec, test_rec],# recall of both train and test\n",
    "                          'f1': [train_f1, test_f1]\n",
    "                         }\n",
    "            #print(new_result)\n",
    "            save_results(new_result, \"by_n_features\")\n",
    "    elif len(argv) == 6:\n",
    "        #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(new_data, isfrom):\n",
    "    # result file is not exits\n",
    "    result_file = isfrom+\".json\"\n",
    "    if not os.path.exists(result_file):\n",
    "        temp = {\"results\": []}\n",
    "        with open(result_file, 'w+') as json_result:\n",
    "            json.dump(temp, json_result)\n",
    "\n",
    "    with open(result_file) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        temp = data[\"results\"]\n",
    "        # python object to be appended\n",
    "        temp.append(new_data)\n",
    "\n",
    "    # then update data\n",
    "    with open(result_file, 'w') as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 7, 'count': 1, 'train_avg_acc': 0.8314014752370916, 'test_avg_acc': 0.6501580611169653}\n",
      "{'_id': 6, 'count': 1, 'train_avg_acc': 0.8103266596417281, 'test_avg_acc': 0.6090621707060063}\n",
      "{'_id': 9, 'count': 1, 'train_avg_acc': 0.7987355110642782, 'test_avg_acc': 0.6564805057955743}\n",
      "{'_id': 4, 'count': 1, 'train_avg_acc': 0.8103266596417281, 'test_avg_acc': 0.6343519494204426}\n",
      "{'_id': 3, 'count': 1, 'train_avg_acc': 0.7924130663856691, 'test_avg_acc': 0.6195995785036881}\n",
      "{'_id': 0, 'count': 1, 'train_avg_acc': 0.827186512118019, 'test_avg_acc': 0.6585879873551106}\n",
      "{'_id': 1, 'count': 1, 'train_avg_acc': 0.8229715489989463, 'test_avg_acc': 0.6649104320337197}\n",
      "{'_id': 2, 'count': 1, 'train_avg_acc': 0.8356164383561644, 'test_avg_acc': 0.6469968387776607}\n",
      "{'_id': 5, 'count': 1, 'train_avg_acc': 0.8029504741833509, 'test_avg_acc': 0.6596417281348789}\n",
      "{'_id': 8, 'count': 1, 'train_avg_acc': 0.8008429926238145, 'test_avg_acc': 0.660695468914647}\n"
     ]
    }
   ],
   "source": [
    "for i in d:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': '20ng_CG_RM', 'count': 10, 'train_avg_acc': 0.8388211382113822, 'test_avg_acc': 0.8179695431472082}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = [{'$match': {'dataset': '20ng_CG_RM', 'k_fold': 1, 'd2v_vec_size':10,\n",
    "                 'algorithm':'lr_mvdt', 'epochs':50, 'min_leaf_point':5, 'feature_size':2\n",
    "                    }\n",
    "         },\n",
    "        {'$group': {'_id':'$dataset', 'count': {'$sum':1},\n",
    "                'train_avg_acc': { '$avg': {'$arrayElemAt': [ '$accuracy', 0 ]}},\n",
    "                'test_avg_acc': { '$avg': { '$arrayElemAt': ['$accuracy', 1]}},\n",
    "                #'k_fold': {'$push': '$k_fold'}\n",
    "                   }\n",
    "        },\n",
    "        {'$project': {'count':1, 'k_fold':1, 'd2v_vec_size':1, 'train_avg_acc':1, 'test_avg_acc':1\n",
    "                     }\n",
    "        }]\n",
    "d = db.result.aggregate(query)\n",
    "for i in d:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': '20ng_CG_RM',\n",
       " 'count': 10,\n",
       " 'train_avg_acc': 0.8388211382113822,\n",
       " 'test_avg_acc': 0.8179695431472082}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results.json\") as json_file: \n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20ng_CG_RM\n",
      "20newsgroup\n",
      "20ng_CG_RM\n",
      "20newsgroup\n"
     ]
    }
   ],
   "source": [
    "#get average accuracy by dataset\n",
    "n = 0\n",
    "total_train = 0\n",
    "total_test = 0\n",
    "dataset = \"\"\n",
    "for i, result in enumerate(data[\"results\"]):\n",
    "    n = i      \n",
    "    #print(dataset)\n",
    "    if dataset != result[\"dataset\"]:        \n",
    "        dataset = result[\"dataset\"]\n",
    "        #datset = result[\"dataset\"]\n",
    "        print(dataset)\n",
    "#    total_train += result[\"accuracy\"][0]\n",
    "#    total_test += result[\"accuracy\"][1]  \n",
    "\n",
    "#print(total_train/(n+1))\n",
    "#print(total_test/(n+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dataset', 'filename', 'k_fold', 'd2v_vec_size', 'algorithm', 'epochs', 'min_leaf_point', 'feature_size', 'd2v_shape', 'run', 'accuracy', 'precision', 'recall', 'f1', 'confusion_matrix', 'max_depth', 'inner_node', 'leaf_node', 'all_node', 'train_true_predict', 'test_true_predict', 'branch_sizes', 'training_time', 'tree_location', 'intermediate_result'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"results\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lets take average\n",
    "dataset, algorithm, accuracy\n",
    "\n",
    "10_run = 10 run average\n",
    "fet = all feature_size, 10_trun\n",
    "mlp = min_leaf_point,  fet\n",
    "ep = epochs, mlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_min_depth(data):\n",
    "    max_depth = 0\n",
    "    max_min_depth = 1\n",
    "    for result in data[\"results\"]:\n",
    "        if result['max_depth'] > max_depth:\n",
    "            max_depth = result['max_depth']\n",
    "        if result['max_depth'] < max_min_depth:\n",
    "            max_min_depth = result['max_depth']\n",
    "            print(result['max_depth'])        \n",
    "\n",
    "    return max_depth, max_min_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_max_min_depth(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_acc_by(data, filed_name=\"epochs\"):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(data[\"results\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count based onalgorith, dataset\n",
    "# then using other hyperparameters\n",
    "# based on that find average accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"20ng_CG_RM\",\"20newsgroup\"]\n",
    "algoriths = [\"lr_mvdt\", \"rs_mvdt\", \"cart\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "count = 0 \n",
    "for results in data[\"results\"]:\n",
    "    for dataset in datasets:        \n",
    "        if results[\"dataset\"] == dataset:\n",
    "            print(dataset)\n",
    "            count += 1\n",
    "            d[results[\"dataset\"]] = count\n",
    "        \n",
    "        #print(results[\"dataset\"])\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[\"results\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dataset', 'filename', 'k_fold', 'd2v_vec_size', 'algorithm', 'epochs', 'min_leaf_point', 'feature_size', 'd2v_shape', 'run', 'accuracy', 'precision', 'recall', 'f1', 'confusion_matrix', 'max_depth', 'inner_node', 'leaf_node', 'all_node', 'train_true_predict', 'test_true_predict', 'branch_sizes', 'training_time', 'tree_location', 'intermediate_result'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"results\"][2].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"results\"][2][\"intermediate_result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
