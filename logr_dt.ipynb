{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an approach of combining logistic regression with decision tree\n",
    "# use logit function to get random or best coefficient of each feature then split\n",
    "# then measure split goodness with entropy/impurity\n",
    "# build tree on best split, take split dataset and repeat the process again\n",
    "# till some criteria meets, like max depth or minimum point for split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression as logit\n",
    "from sklearn.model_selection import train_test_split as tts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data as dataframe\n",
    "data = \"name of data.csv\" sf\n",
    "\n",
    "df = pd.read_csv(\"data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "epochs = 1000\n",
    "tree_depth = 4\n",
    "min_pts = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in train and test\n",
    "X_train, y_train, X_test, y_test = tts(df, split=0.7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy():\n",
    "    return entropy_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a funciton to run \n",
    "def best_theta(dataset):\n",
    "    # initilize algo here because it has to fit in every node\n",
    "    logit_clf = logit()\n",
    "    \n",
    "    logit_clf.fit(X_train, y_train)\n",
    "    \n",
    "    return logit_clf.coeff__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with coefficent check for loss/cost funciton of entropy\n",
    "def measure_error(dataset):\n",
    "    # get theta\n",
    "    [theta] = best_theta(dataset)\n",
    "    \n",
    "    # find total entropy of given dataset\n",
    "    total_entropy = entropy()\n",
    "    # find check its error with ture value\n",
    "    pos_ent = entropy()\n",
    "    neg_ent = entropy()\n",
    "    \n",
    "    # information gain\n",
    "    gain = total_entropy - all entropy\n",
    "    \n",
    "    return gain, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for best gain and split the data\n",
    "a bit hard or find a way of what? because there is no random point selection because there is no reason to check everytime\n",
    "or I hava to check in depth how it works. or There is random way of finding theta with in gradient descend \n",
    "\n",
    "call here split data build the tree repeat this process whole again until some criteria\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}